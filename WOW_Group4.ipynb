{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Financial Inclusion in Africa**"
      ],
      "metadata": {
        "id": "lptiKlnDanWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this notebook is to create a machine learning model to predict which individuals are most likely to have or use a bank account. The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda."
      ],
      "metadata": {
        "id": "D8R4pKx5a8rI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5hxWNx7bW1i"
      },
      "source": [
        "# **BLUFF**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Running the various classifier models the LightGBM model had the highest AUC of 80.9%. Following close behind was the XGBoost model with an AUC of 80.7%. Our models were compared to the output of a driverless AI model developed by https://h2o.ai/H20.ai. The comparison can be found in our written report."
      ],
      "metadata": {
        "id": "W8Tpis67bWTJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8167fZuR8t0"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veKsgrEaZSvj"
      },
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import pyplot\n",
        "import scipy.stats as ss\n",
        "import math\n",
        "from scipy.stats.mstats import winsorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1J-BxhWIrP2"
      },
      "outputs": [],
      "source": [
        "# sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import metrics\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, StackingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H05sPIaVYOj"
      },
      "outputs": [],
      "source": [
        "# sklearn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, log_loss, balanced_accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.inspection import partial_dependence\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U881phGzE9sE"
      },
      "outputs": [],
      "source": [
        "# mblearn library\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uvVE1BahE9gM",
        "outputId": "3aa973e1-3f3a-427e-84a7-f15b813b9f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shapash\n",
            "  Downloading shapash-2.0.1-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting dash-daq>=0.5.0\n",
            "  Downloading dash_daq-0.5.0.tar.gz (642 kB)\n",
            "\u001b[K     |████████████████████████████████| 642 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting shap>=0.38.1\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 36.2 MB/s \n",
            "\u001b[?25hCollecting dash-html-components>=2.0.0\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting numba>=0.53.1\n",
            "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from shapash) (3.2.2)\n",
            "Collecting dash-table>=5.0.0\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting dash>=2.3.1\n",
            "  Downloading dash-2.4.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 17.8 MB/s \n",
            "\u001b[?25hCollecting dash-bootstrap-components>=1.1.0\n",
            "  Downloading dash_bootstrap_components-1.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting scikit-learn<=0.24.2,>=0.24.0\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 11.5 MB/s \n",
            "\u001b[?25hCollecting dash-core-components>=2.0.0\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-renderer==1.8.3\n",
            "  Downloading dash_renderer-1.8.3.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from shapash) (5.5.0)\n",
            "Requirement already satisfied: pandas>1.0.2 in /usr/local/lib/python3.7/dist-packages (from shapash) (1.3.5)\n",
            "Requirement already satisfied: nbformat>4.2.0 in /usr/local/lib/python3.7/dist-packages (from shapash) (5.4.0)\n",
            "Requirement already satisfied: numpy>1.18.0 in /usr/local/lib/python3.7/dist-packages (from shapash) (1.21.6)\n",
            "Collecting flask-compress\n",
            "  Downloading Flask_Compress-1.12-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from dash>=2.3.1->shapash) (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=2.3.1->shapash) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=2.3.1->shapash) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=2.3.1->shapash) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=2.3.1->shapash) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=2.3.1->shapash) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->shapash) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->shapash) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->shapash) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->shapash) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->shapash) (4.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>4.2.0->shapash) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>4.2.0->shapash) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>4.2.0->shapash) (4.3.3)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from nbformat>4.2.0->shapash) (5.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>4.2.0->shapash) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>4.2.0->shapash) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>4.2.0->shapash) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>4.2.0->shapash) (4.11.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>4.2.0->shapash) (3.8.0)\n",
            "Collecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 6.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53.1->shapash) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>1.0.2->shapash) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->shapash) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->shapash) (8.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<=0.24.2,>=0.24.0->shapash) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<=0.24.2,>=0.24.0->shapash) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<=0.24.2,>=0.24.0->shapash) (3.1.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap>=0.38.1->shapash) (1.3.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap>=0.38.1->shapash) (21.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap>=0.38.1->shapash) (4.64.0)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 40.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: dash-renderer, dash-daq\n",
            "  Building wheel for dash-renderer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-renderer: filename=dash_renderer-1.8.3-py3-none-any.whl size=1013944 sha256=1b943a4857f50915b35162d07823d8b9361e336fddb23a064e929ef52548be3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/17/b0/8bebc086e55a01cf4036902b6ca05fce3fb4c7fd48dbd07717\n",
            "  Building wheel for dash-daq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-daq: filename=dash_daq-0.5.0-py3-none-any.whl size=669714 sha256=644d810e8fdb076a29bf8e55e8fc69fbd2779ddcc1db4e7c92c3b0a27f5399ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/54/53/a8d448db5592874db4313240571ca2c069e55f6a6b29bf5847\n",
            "Successfully built dash-renderer dash-daq\n",
            "Installing collected packages: brotli, llvmlite, flask-compress, dash-table, dash-html-components, dash-core-components, slicer, scikit-learn, numba, dash, shap, dash-renderer, dash-daq, dash-bootstrap-components, shapash\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed brotli-1.0.9 dash-2.4.1 dash-bootstrap-components-1.1.0 dash-core-components-2.0.0 dash-daq-0.5.0 dash-html-components-2.0.0 dash-renderer-1.8.3 dash-table-5.0.0 flask-compress-1.12 llvmlite-0.38.0 numba-0.55.1 scikit-learn-0.24.2 shap-0.40.0 shapash-2.0.1 slicer-0.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install shapash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyC1N_CEFAVy",
        "outputId": "42e25723-88ed-4129-c9f6-877f824c8f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 275 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.24.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.18.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->lime) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=df4054f58c5e1faf12ef14a5091f28a2af6f172a0e02b309145851ab7df84b06\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lime\n",
        "import lime\n",
        "import lime.lime_tabular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfwIasvQFAQ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "05739492-f26e-4e8d-e0da-393041b80057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdpbox\n",
            "  Downloading PDPbox-0.2.1.tar.gz (34.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.0 MB 184 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pdpbox) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pdpbox) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pdpbox) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pdpbox) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pdpbox) (5.4.8)\n",
            "Collecting matplotlib==3.1.1\n",
            "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 22.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pdpbox) (0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->pdpbox) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->pdpbox) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->pdpbox) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->pdpbox) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.1->pdpbox) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.1->pdpbox) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pdpbox) (2022.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pdpbox) (0.24.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pdpbox) (3.1.0)\n",
            "Building wheels for collected packages: pdpbox\n",
            "  Building wheel for pdpbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdpbox: filename=PDPbox-0.2.1-py3-none-any.whl size=35758224 sha256=766bf1510e06815f465d81214523ef4ddb5ebb9deaa5defe72973650483ca163\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/d0/1a/b80035625c53131f52906a6fc4dd690d8efd2bf8af6a4015eb\n",
            "Successfully built pdpbox\n",
            "Installing collected packages: matplotlib, pdpbox\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "shapash 2.0.1 requires matplotlib>=3.2.0, but you have matplotlib 3.1.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.1 pdpbox-0.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pdpbox\n",
        "from pdpbox import pdp, get_dataset, info_plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpdO42dNNZ5j",
        "outputId": "76962ecf-4b15-4b0a-a5e8-829489cadca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H595lbNpNffU"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwiDMSuo589m"
      },
      "outputs": [],
      "source": [
        "# Keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS3PDE9Y0UJD",
        "outputId": "6246238c-a567-4957-8fec-7080f8212af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (0.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# LightGBM Library\n",
        "!pip install lightgbm\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItkAf9WvSW5K"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etwPjjqrUCJ3",
        "outputId": "45dcfcea-4767-4d5c-d44b-0e617cf906b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openpyxl==3.0.5\n",
            "  Downloading openpyxl-3.0.5-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 102 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 112 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 122 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 133 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 153 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 163 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 184 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 194 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 204 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 215 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 225 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 235 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 242 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl==3.0.5) (1.1.0)\n",
            "Collecting jdcal\n",
            "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jdcal, openpyxl\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 3.0.9\n",
            "    Uninstalling openpyxl-3.0.9:\n",
            "      Successfully uninstalled openpyxl-3.0.9\n",
            "Successfully installed jdcal-1.4.1 openpyxl-3.0.5\n"
          ]
        }
      ],
      "source": [
        "# Update openpyxl\n",
        "!pip install openpyxl==3.0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3csKoIexScnk",
        "outputId": "78ff21d4-de62-4a5a-b619-58ab683a4e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl==3.0.5 in /usr/local/lib/python3.7/dist-packages (3.0.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl==3.0.5) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl==3.0.5) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "# Update openpyxl\n",
        "!pip install openpyxl==3.0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLNhyFIONt5f",
        "outputId": "d9f71964-9e61-4eaf-92a7-7bcf8973aa6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read \n",
        "file_ = \"drive/My Drive/Colab Notebooks/AfricaTrainData.csv\"   # adapt this as needed to the file structure on your Google drive\n",
        "df = pd.read_csv(file_) # read in csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEZu7qRsTeYL"
      },
      "source": [
        "# **EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KMrKzVNw3OJ",
        "outputId": "4f251571-db7d-408e-9486-c0f575c91b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape (23524, 13)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23524 entries, 0 to 23523\n",
            "Data columns (total 13 columns):\n",
            " #   Column                  Non-Null Count  Dtype \n",
            "---  ------                  --------------  ----- \n",
            " 0   country                 23524 non-null  object\n",
            " 1   year                    23524 non-null  int64 \n",
            " 2   uniqueid                23524 non-null  object\n",
            " 3   bank_account            23524 non-null  object\n",
            " 4   location_type           23524 non-null  object\n",
            " 5   cellphone_access        23524 non-null  object\n",
            " 6   household_size          23524 non-null  int64 \n",
            " 7   age_of_respondent       23524 non-null  int64 \n",
            " 8   gender_of_respondent    23524 non-null  object\n",
            " 9   relationship_with_head  23524 non-null  object\n",
            " 10  marital_status          23524 non-null  object\n",
            " 11  education_level         23524 non-null  object\n",
            " 12  job_type                23524 non-null  object\n",
            "dtypes: int64(3), object(10)\n",
            "memory usage: 2.3+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Shape of df\n",
        "print(\"Shape\", df.shape)\n",
        "\n",
        "# Check data types\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf-DvEEoIyR7"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MWiONPDVBnu",
        "outputId": "5da546da-363e-4f35-d0d8-e5eb260b223d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of dups: 0\n"
          ]
        }
      ],
      "source": [
        "df.drop_duplicates(keep=False, inplace=True)\n",
        "duplicates_count = df.duplicated().sum()\n",
        "print('No. of dups:',duplicates_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7sf3Dy2VBlQ"
      },
      "outputs": [],
      "source": [
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        " \n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        " \n",
        "# Encode labels in column 'species'.\n",
        "df['relationship_with_head']= label_encoder.fit_transform(df['relationship_with_head'])\n",
        "df['marital_status']= label_encoder.fit_transform(df['marital_status'])\n",
        "df['education_level']= label_encoder.fit_transform(df['education_level'])\n",
        "df['job_type']= label_encoder.fit_transform(df['job_type'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAIMJVGvVBio"
      },
      "outputs": [],
      "source": [
        "# create dummy variables for categorical variables \n",
        "df = pd.get_dummies(data=df, columns=['location_type', 'cellphone_access', 'gender_of_respondent'],drop_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iziw0cTIVBgH"
      },
      "outputs": [],
      "source": [
        "# Encode categorical target variable \n",
        "df.bank_account.replace(('Yes', 'No'), (1, 0), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5d1eQ1yVIZ0",
        "outputId": "aba0c93e-4a16-40b6-b6a2-d1ce8e49c7ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    20212\n",
              "1     3312\n",
              "Name: bank_account, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df['bank_account'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBut1PjAVBdq"
      },
      "outputs": [],
      "source": [
        "# pick numerical variables and set them as X\n",
        "X = df[['household_size','age_of_respondent']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwMQo1bcVBbB"
      },
      "outputs": [],
      "source": [
        "# Code for skewness correction (see source below)\n",
        "# Depending upon the characteritics of a feature (column), a log, Box-Cox or power transform is applied to normalize the distribution \n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Feb 23 14:42:46 2019\n",
        "@author: DATAmadness\n",
        "\"\"\"\n",
        "\n",
        "##################################################\n",
        "# A function that will accept a pandas dataframe\n",
        "# and auto-transforms columns that exceeds threshold value\n",
        "#  -  Offers choice between boxcox or log / exponential transformation\n",
        "#  -  Automatically handles negative values\n",
        "#  -  Auto recognizes positive /negative skewness\n",
        "\n",
        "# Further documentation available here:\n",
        "# https://datamadness.github.io/Skewness_Auto_Transform\n",
        "\n",
        "def skew_autotransform(DF, include = None, exclude = None, plot = False, threshold = 1, exp = False):\n",
        "    \n",
        "    #Get list of column names that should be processed based on input parameters\n",
        "    if include is None and exclude is None:\n",
        "        colnames = DF.columns.values\n",
        "    elif include is not None:\n",
        "        colnames = include\n",
        "    elif exclude is not None:\n",
        "        colnames = [item for item in list(DF.columns.values) if item not in exclude]\n",
        "    else:\n",
        "        print('No columns to process!')\n",
        "    \n",
        "    #Helper function that checks if all values are positive\n",
        "    def make_positive(series):\n",
        "        minimum = np.amin(series)\n",
        "        #If minimum is negative, offset all values by a constant to move all values to positive teritory\n",
        "        if minimum <= 0:\n",
        "            series = series + abs(minimum) + 0.01\n",
        "        return series\n",
        "    \n",
        "    \n",
        "    #Go through desired columns in DataFrame\n",
        "    for col in colnames:\n",
        "        #Get column skewness\n",
        "        skew = DF[col].skew()\n",
        "        transformed = True\n",
        "        \n",
        "        if plot:\n",
        "            #Prep the plot of original data\n",
        "            sns.set_style(\"darkgrid\")\n",
        "            sns.set_palette(\"Blues_r\")\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            #ax1 = sns.distplot(DF[col], ax=axes[0])\n",
        "            ax1 = sns.histplot(DF[col], ax=axes[0], color=\"blue\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n",
        "            ax1.set(xlabel='Original ' + str(col))\n",
        "        \n",
        "        #If skewness is larger than threshold and positively skewed; If yes, apply appropriate transformation\n",
        "        if abs(skew) > threshold and skew > 0:\n",
        "            skewType = 'positive'\n",
        "            #Make sure all values are positive\n",
        "            DF[col] = make_positive(DF[col])\n",
        "            \n",
        "            if exp:\n",
        "               #Apply log transformation \n",
        "               DF[col] = DF[col].apply(math.log)\n",
        "            else:\n",
        "                #Apply boxcox transformation\n",
        "                DF[col] = ss.boxcox(DF[col])[0]\n",
        "            skew_new = DF[col].skew()\n",
        "         \n",
        "        elif abs(skew) > threshold and skew < 0:\n",
        "            skewType = 'negative'\n",
        "            #Make sure all values are positive\n",
        "            DF[col] = make_positive(DF[col])\n",
        "            \n",
        "            if exp:\n",
        "               #Apply exp transformation \n",
        "               DF[col] = DF[col].pow(10)\n",
        "            else:\n",
        "                #Apply boxcox transformation\n",
        "                DF[col] = ss.boxcox(DF[col])[0]\n",
        "            skew_new = DF[col].skew()\n",
        "        \n",
        "        else:\n",
        "            #Flag if no transformation was performed\n",
        "            transformed = False\n",
        "            skew_new = skew\n",
        "        \n",
        "        #Compare before and after if plot is True\n",
        "        if plot:\n",
        "            print('\\n ------------------------------------------------------')     \n",
        "            if transformed:\n",
        "                print('\\n %r had %r skewness of %2.2f' %(col, skewType, skew))\n",
        "                print('\\n Transformation yielded skewness of %2.2f' %(skew_new))\n",
        "                sns.set_palette(\"Paired\")\n",
        "                #ax2 = sns.distplot(DF[col], ax=axes[1], color = 'r')\n",
        "                ax2 = sns.histplot(DF[col], ax=axes[1], color=\"red\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n",
        "                ax2.set(xlabel='Transformed ' + str(col))\n",
        "                plt.show()\n",
        "            else:\n",
        "                print('\\n NO TRANSFORMATION APPLIED FOR %r . Skewness = %2.2f' %(col, skew))\n",
        "                #ax2 = sns.distplot(DF[col], ax=axes[1])\n",
        "                ax2 = sns.histplot(DF[col], ax=axes[1], color=\"blue\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n",
        "                ax2.set(xlabel='NO TRANSFORM ' + str(col))\n",
        "                plt.show()\n",
        "                \n",
        "\n",
        "    return DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQzs2lVzVBSu"
      },
      "outputs": [],
      "source": [
        "# Use code above (adapted from https://github.com/datamadness/Automatic-skewness-transformation-for-Pandas-DataFrame) to correct skewness\n",
        "# All the predictors are real-valued, so we can push them all through the skewness check/correction.\n",
        "X = skew_autotransform(X.copy(deep=True), plot = False, exp = False, threshold = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2elXkJUPVMym"
      },
      "outputs": [],
      "source": [
        "# Tukey Rule outliers\n",
        "# As an alternative, you could use z-scores greater than 3 or less than -3.\n",
        "\n",
        "cols = X.columns\n",
        "#Tukey's method\n",
        "def tukey_rule(data, col):\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_lim = data[col].quantile(0.5) + 2 * IQR\n",
        "    lower_lim = data[col].quantile(0.5) - 2 * IQR\n",
        "    outliers = []\n",
        "    for index, x in enumerate(data[col]):\n",
        "        if x < lower_lim or x >= upper_lim:\n",
        "            outliers.append(index)\n",
        "    return outliers\n",
        "\n",
        "# Identify outliers\n",
        "for i in cols:\n",
        "  outliers_Tukey = tukey_rule(X,i)\n",
        "  \n",
        "# Windsorize X and check the results\n",
        "X_winsorized = X.copy(deep=True)\n",
        "for i in cols:\n",
        "  X_winsorized[i] = winsorize(X[i], limits=(0.05, 0.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se91OjFwVODG"
      },
      "outputs": [],
      "source": [
        "# Update result df with winzorized quant featires\n",
        "df[['household_size']]=X_winsorized[['household_size']]\n",
        "df[['age_of_respondent']]=X_winsorized[['age_of_respondent']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2M2ifMi4JU9"
      },
      "outputs": [],
      "source": [
        "# drop the column \"year\" ,\"country\" and \"uniqueid\"\n",
        "df = df.drop([\"year\",\"uniqueid\",\"country\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee0cZVGMAU-y",
        "outputId": "d8e3d858-874b-4a3b-a336-f98089ff407f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['bank_account', 'household_size', 'age_of_respondent',\n",
              "       'relationship_with_head', 'marital_status', 'education_level',\n",
              "       'job_type', 'location_type_Urban', 'cellphone_access_Yes',\n",
              "       'gender_of_respondent_Male'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Confirm that the columsn match teh dictionary!\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhAzzffDfIwo"
      },
      "source": [
        "# **Split Predictors & Target**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0-mfxcPFs4W"
      },
      "outputs": [],
      "source": [
        "# Split Predictors\n",
        "X = df.drop(['bank_account'],axis=1)              \n",
        "y = df ['bank_account']  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF224zMsfa1w"
      },
      "source": [
        "# **Create Holdout Sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjwm2uUcGWiP"
      },
      "outputs": [],
      "source": [
        "# Split instances into training and test set, i.e., create holdout sample\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=54321, stratify=y)    # This is a sklearn method. The test_size specifies the fraction of the dataset that\n",
        "                                                                                                             # goes into the test set. stratify=1 hyperparameter ensures that the training and tests sets have the \n",
        "                                                                                                             # same balance (i.e., the same proportion of positive and negative target values)\n",
        "                                                                                                             # random_state fixes the seed of the random number generator, so that the split is the same every time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6F8fJE4frPr"
      },
      "source": [
        "# **SMOTE (Oversampling)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTWFaqauGX0e",
        "outputId": "505259e2-34c1-48a7-e9bd-9a0ac7317ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before SMOTE:  (18819, 9) (18819,)\n",
            "Shape after SMOTE:  (32338, 9) (32338,)\n",
            "Mean of target:  0.5\n"
          ]
        }
      ],
      "source": [
        "# SMOTE (oversampling)\n",
        "import imblearn                                                      # Import the imblearn package. sklearn does not feature SMOTE\n",
        "from imblearn.over_sampling import SMOTE                             # Import SMOTE\n",
        "\n",
        "sm = SMOTE(random_state=12346)                                       # Create an instance of SMOTE\n",
        "X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train, y_train)     # Apply the SNOTE intance to the training data\n",
        "\n",
        "print(\"Shape before SMOTE: \", X_train.shape, y_train.shape)          # Verify the shape has changed and is now balanced.\n",
        "print(\"Shape after SMOTE: \", X_train_SMOTE.shape, y_train_SMOTE.shape)\n",
        "print(\"Mean of target: \",mean(y_train_SMOTE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YFp7l7ui0Uc"
      },
      "outputs": [],
      "source": [
        "Xcols= X.columns\n",
        "col_names=X.columns\n",
        "cols=df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loC9WpZLghQy"
      },
      "source": [
        "# **Standardize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROhlOjRpGXxk"
      },
      "outputs": [],
      "source": [
        "# Standardize\n",
        "mmsc = StandardScaler()                                   # Create an instance of MinMaxScaler\n",
        "X_train_SMOTE_std = mmsc.fit_transform(X_train_SMOTE)   # Uses .fit_transform to compute the mean and standard deviaton of training dataset and to use those statistics to nromalize the training data\n",
        "X_test_std = mmsc.transform(X_test)                     # Uses .transform to apply the training set transformation to the test set. This way both datasets are normalized similarly, but \n",
        "                                                        # no information from the test set is leaked back into the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeiLJW6xgpKl"
      },
      "source": [
        "# **Support Vector Machine Classifier (SVC)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSANZpVCGb20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a91c29-5592-4bc2-9af7-e1fd274f9b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix: \n",
            " [[2901 1142]\n",
            " [ 155  507]]\n",
            "AUC:  0.7416987550000635\n"
          ]
        }
      ],
      "source": [
        "# Build Random Forest classifer on revised dataset\n",
        "from sklearn.metrics import confusion_matrix                                # Import confusion_matrix method from sklearn\n",
        "from sklearn import metrics                                                 # Import metrics from sklearn\n",
        "model = SVC(kernel='rbf')                                                   # Create a Support Vector Machine Classifier\n",
        "model.fit(X_train_SMOTE_std,y_train_SMOTE)                                  # Fit the revised training data (i.e., the training data has only five features)\n",
        "y_pred = model.predict(X_test_std)                                          # Use model to predict target values for test predictors\n",
        "print('Confusion matrix: \\n',confusion_matrix(y_test, y_pred))              # print confusion matrix using sklearn \n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)       # Compute true positive rate and false positive rate using the roc_curve method\n",
        "print('AUC: ', metrics.auc(fpr, tpr))                                       # Compute and print the area under the ROC curve, AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTvRoNJ7WL9j"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=model,\n",
        "                               X=X_train_SMOTE_std,\n",
        "                               y=y_train_SMOTE,\n",
        "                               train_sizes=np.linspace(0.5, 1.0, 5),\n",
        "                               cv=10,\n",
        "                               n_jobs=1)\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "\n",
        "print('Test Mean: ',test_mean)\n",
        "plt.plot(train_sizes, train_mean,\n",
        "         color='blue', marker='o',\n",
        "         markersize=5, label='training accuracy')\n",
        "\n",
        "plt.fill_between(train_sizes,\n",
        "                 train_mean + train_std,\n",
        "                 train_mean - train_std,\n",
        "                 alpha=0.15, color='blue')\n",
        "\n",
        "plt.plot(train_sizes, test_mean,\n",
        "         color='green', linestyle='--',\n",
        "         marker='s', markersize=5,\n",
        "         label='validation accuracy')\n",
        "\n",
        "plt.fill_between(train_sizes,\n",
        "                 test_mean + test_std,\n",
        "                 test_mean - test_std,\n",
        "                 alpha=0.15, color='green')\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel('Number of training samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim([0.3, 1.00])\n",
        "plt.tight_layout()\n",
        "plt.title('Learning Curve')\n",
        "# plt.savefig('./figures/learning_curve.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGvhDtOmuYqC"
      },
      "source": [
        "# **Permutation Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "781WwMtvr6eP"
      },
      "outputs": [],
      "source": [
        "# Assess features important for predictions in the wild\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
        "perm_importance = permutation_importance(model, X_test_std, y_test)\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "plt.barh(Xcols[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
        "plt.xlabel(\"Permutation Importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uHS8IG4v4Fq"
      },
      "source": [
        "# **Feature Importance Using XGBoost Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzSRHsqVztJQ"
      },
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(n_estimators=100)\n",
        "xgb.fit(X_train_SMOTE, np.ravel(y_train_SMOTE))\n",
        "sorted_idx = xgb.feature_importances_.argsort()\n",
        "plt.barh(cols[sorted_idx], xgb.feature_importances_[sorted_idx])\n",
        "plt.xlabel(\"XGBoost Model Feature Importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTIEfUnowIYC"
      },
      "source": [
        "# **Feature Importance Using Random Forest Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3nunfTNsft1"
      },
      "outputs": [],
      "source": [
        "# Fit Random Forest model, recover feature importances\n",
        "# Assess features important in the model-building process\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_SMOTE_std,np.ravel(y_train_SMOTE))\n",
        "sorted_idx = rf.feature_importances_.argsort()\n",
        "plt.barh(Xcols[sorted_idx], rf.feature_importances_[sorted_idx])\n",
        "plt.xlabel(\"Random Forest Model Feature Importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRZi8v7AwTPd"
      },
      "source": [
        "# **KDE Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiXI606qxxr2"
      },
      "outputs": [],
      "source": [
        "# Generate kde plots to examine the density functions of the most important features\n",
        "y_test=pd.DataFrame(y_test)\n",
        "y.columns=['first_trip_tz']\n",
        "X_test_std = pd.DataFrame(X_test_std)\n",
        "X_test_std.columns=Xcols\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(6, 6), dpi=80)\n",
        "df=pd.concat([y_test,X_test_std],axis=1)\n",
        "for i in Xcols:\n",
        "    sns.kdeplot(df.loc[(df['bank_account']==1),i], color='red', shade=True, Label='bank_account')\n",
        "    sns.kdeplot(df.loc[(df['bank_account']==0),i], color='blue', shade=True, Label='not')\n",
        "    plt.xlabel('Feature: '+str(i))\n",
        "    plt.ylabel('Proportion')   \n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH_vI55Gwn-b"
      },
      "source": [
        "# **Partial Dependence Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1o0c7Omy4Lx"
      },
      "outputs": [],
      "source": [
        "# Univariate Partial Dependence Plot\n",
        "for i in Xcols:    \n",
        "    pdp_ = pdp.pdp_isolate(model = model, dataset = X_test_std, model_features=Xcols, feature = i)\n",
        "    pdp.pdp_plot(pdp_, str(i))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzR5BaqFx7rY"
      },
      "outputs": [],
      "source": [
        "Xcols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H0-d77ix-EZ"
      },
      "outputs": [],
      "source": [
        "# Bivariate PDP\n",
        "# Similar to previous PDP plots except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
        "if 'location_type_Urban' in Xcols and 'marital_status' in Xcols:\n",
        "  features_to_plot = ['location_type_Urban', 'marital_status']\n",
        "  inter1  =  pdp.pdp_interact(model=model, dataset=X_test_std, model_features=Xcols, features=features_to_plot)\n",
        "  pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_F7w-W_QX0ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzusAi9f1OeZ"
      },
      "source": [
        "# **Surrogate Models (Global)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjedF6B43b4g"
      },
      "outputs": [],
      "source": [
        "# Decision Tree surrogate model of Random Forest model\n",
        "# Mimics the beahvior of the black box model on data from the wild (i.e., the test set)\n",
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "proxy = DecisionTreeClassifier(random_state = 20850,max_depth=2)    # Control the depth of the proxy tree here\n",
        "proxy.fit(X_test,y_pred)\n",
        "\n",
        "tree_graph = tree.export_graphviz(proxy, out_file = None, feature_names = Xcols)\n",
        "graphviz.Source(tree_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVdZiGCz_dH0"
      },
      "outputs": [],
      "source": [
        "# How good is this surrogate decision tree model? \n",
        "# Let's find out by computing the correlation between the predictions of the original model and the surrogate model\n",
        "y_proxy = proxy.predict(X_test) # Use the decision tree to make predictions\n",
        "y_proxy = pd.DataFrame(y_proxy)\n",
        "y_pred=pd.DataFrame(y_pred)\n",
        "print('Correlation coefficient of RF predictions and Surrogate Model predictions: ',y_pred.corrwith(y_proxy,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try other classifiers\n",
        "\n",
        "names = [\"Decision Tree\", \"k Nearest Neighbors\", \"SVM\", \"MLP\", \"Random Forest\", \"XGBoost\", \"Light GBM\"]\n",
        "classifiers = [ \n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(),\n",
        "    MLPClassifier(hidden_layer_sizes=(20,20),alpha=1, max_iter=500),\n",
        "    RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=100, max_depth=3),\n",
        "    XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "                  n_estimators=100, max_depth=3),\n",
        "    LGBMClassifier(boosting_type='gbdt', objective='binary', num_leaves=50,\n",
        "                                learning_rate=0.1, bagging_fraction=0.9, feature_fraction=0.9, reg_lambda=0.2)]\n",
        "\n",
        "for name, clf in zip(names, classifiers):\n",
        "  pipe_many = make_pipeline(StandardScaler(),\n",
        "                        PCA(n_components=5),\n",
        "                        clf)\n",
        "  scores = cross_val_score(estimator=pipe_many,\n",
        "                             X=X_train,\n",
        "                             y=np.ravel(y_train),\n",
        "                             cv=10,     #Reduced to three folds for execution speed\n",
        "                             n_jobs=1,\n",
        "                            scoring='roc_auc')\n",
        "  print(\"Classifier: \",name)\n",
        "  print('CV AUC scores: {}'.format(scores))\n",
        "  print('CV AUC mean:{} and std:{}'.format(np.mean(scores), np.std(scores)))\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "uT2o0SqylOY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The leaderboard is now as follows:\n",
        "Classifier performance results are ordered by AUC.\n",
        "\n",
        "|**Classifier** |    AUC    |             |        |\n",
        "|:----------------------|:------------:|:--------------:|:-------------:|\n",
        "|LightGBM | 0.809 |\n",
        "|XGBoost | 0.807 |\n",
        "|MLP | 0.795 |\n",
        "|Decision Tree | 0.758 |\n",
        "|Support Vector Machine | 0.746 |\n",
        "|SVC | 0.743 |\n",
        "|kNN | 0.711 |\n"
      ],
      "metadata": {
        "id": "2l7UgfO2zs72"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "WOW_Group4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}